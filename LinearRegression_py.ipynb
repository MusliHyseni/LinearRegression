{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMB7SIW87V0qM8CbCfCF6Zv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MusliHyseni/LinearRegression/blob/main/LinearRegression_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoDR_ISYloO2",
        "outputId": "06d5592c-9d81-4c2d-e131-5d6572d42b21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3.4172, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 0, loss: 3.4171907901763916\n",
            "tensor(0.6872, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 1, loss: 0.687216579914093\n",
            "tensor(0.1647, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 2, loss: 0.16473305225372314\n",
            "tensor(0.0647, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 3, loss: 0.06473618000745773\n",
            "tensor(0.0456, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 4, loss: 0.04559803009033203\n",
            "tensor(0.0419, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 5, loss: 0.041935235261917114\n",
            "tensor(0.0412, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 6, loss: 0.041234225034713745\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 7, loss: 0.04110005497932434\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 8, loss: 0.04107438027858734\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 9, loss: 0.04106946662068367\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 10, loss: 0.04106852412223816\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 11, loss: 0.04106834530830383\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 12, loss: 0.04106831178069115\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 13, loss: 0.04106830060482025\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 14, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 15, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 16, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 17, loss: 0.04106830060482025\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 18, loss: 0.04106830060482025\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 19, loss: 0.04106830060482025\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 20, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 21, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 22, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 23, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 24, loss: 0.04106830060482025\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 25, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 26, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 27, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 28, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 29, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 30, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 31, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 32, loss: 0.04106830060482025\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 33, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 34, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 35, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 36, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 37, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 38, loss: 0.04106830060482025\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 39, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 40, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 41, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 42, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 43, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 44, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 45, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 46, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 47, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 48, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 49, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 50, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 51, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 52, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 53, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 54, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 55, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 56, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 57, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 58, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 59, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 60, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 61, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 62, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 63, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 64, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 65, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 66, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 67, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 68, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 69, loss: 0.04106830060482025\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 70, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 71, loss: 0.04106830060482025\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 72, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 73, loss: 0.04106830060482025\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 74, loss: 0.04106830060482025\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 75, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 76, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 77, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 78, loss: 0.04106830060482025\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 79, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 80, loss: 0.04106830060482025\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 81, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 82, loss: 0.04106830060482025\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 83, loss: 0.04106830060482025\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 84, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 85, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 86, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 87, loss: 0.04106830060482025\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 88, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 89, loss: 0.04106830060482025\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 90, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 91, loss: 0.04106830060482025\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 92, loss: 0.04106830060482025\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 93, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 94, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 95, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 96, loss: 0.04106830060482025\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 97, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 98, loss: 0.04106830060482025\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 99, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 100, loss: 0.04106830060482025\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 101, loss: 0.04106830060482025\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 102, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 103, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 104, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 105, loss: 0.04106830060482025\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 106, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 107, loss: 0.04106830060482025\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 108, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 109, loss: 0.04106830060482025\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 110, loss: 0.04106830060482025\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 111, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 112, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 113, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 114, loss: 0.04106830060482025\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 115, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 116, loss: 0.04106830060482025\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 117, loss: 0.04106830433011055\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 118, loss: 0.04106830060482025\n",
            "tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
            "Epoch: 119, loss: 0.04106830433011055\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "\n",
        "# The first step is to define the model creator class\n",
        "class LinearRegression(nn.Module):\n",
        "    def __init__(self, inputSize:torch.Tensor, outputSize:torch.Tensor):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(inputSize, outputSize)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "# Then, we instantiate the model\n",
        "model = LinearRegression(6, 1)\n",
        "# define the parameters\n",
        "e = 0.3\n",
        "epochs = 120\n",
        "\n",
        "inputNums = torch.rand(6)\n",
        "weights = torch.rand(6)\n",
        "yNums = torch.tensor([2*i + 1 for i in inputNums])\n",
        "bias = torch.rand(()).item()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "\n",
        "# define the functions\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=e)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    if torch.cuda.is_available():\n",
        "        inputNums = inputNums.cuda()\n",
        "        yNums = yNums.cuda()\n",
        "\n",
        "    # clear the gradient, as we don't want the gradient of the previous epoch to interfere\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = model(inputNums)\n",
        "\n",
        "    loss = loss_function(output, yNums)\n",
        "    print(loss)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    print(f\"Epoch: {epoch}, loss: {loss}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    if torch.cuda.is_available():\n",
        "        predicted = model(inputNums).cuda()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}